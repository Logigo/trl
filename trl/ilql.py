# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02-ppo.ipynb (unless otherwise specified).

__all__ = ['AdaptiveKLController', 'FixedKLController', 'ILQLTrainer']

# Cell
import numpy as np
import torch.nn.functional as F
from torch.optim import Adam
import torch
import collections
import time
import random

from transformers import DataCollatorForLanguageModeling
from trl.gpt2_ilql import CausalLMOutputWithCrossAttentions, GPT2HeadWithQValueModel

from .core import (logprobs_from_logits,
                      whiten,
                      clip_by_value,
                      entropy_from_logits,
                      flatten_dict,
                      average_torch_dicts,
                      stats_to_np,
                      stack_dicts,
                      add_suffix,
                      WANDB_PADDING)


torch.autograd.set_detect_anomaly(True)
# Cell
# 1. ILQLTrainer gets initialized with its parameters, a tokenizer, and a model (M).
# 2. The tokenizer is given a query (sentence) to encode into a Tensor Q_T.
# 3. The model M responds to the encoded query Q_T, and the tokenizer decodes the response R_T.
# 4. There is a reward r. Somehow. 
# 5. The ILQLTrainer calls Step on the query and response tensors (Q_T and R_T) and reward r.
# 6. Step(Q_T, R_T, r) -> Stats:
#   a. Runs batched_forward_pass(Q_T, R_T) to get logprobs, ref_logprobs, and values
#       a.i. For each iteration of size BS/FBS, runs the model M on input of [query, response].
#           to return logits, and values. Runs [query, response] through reference model M_r to return
#              reference logits. TODO: Should this return logits, values, and q, for ILQL?
#       a.ii. Logits and reference logits are turned into logprobs and ref_logprobs. Returns
#           lists of logprobs, ref_logprobs, and values
#   b. Runs compute_rewards(scores=r, logprobs, ref_logprobs) to get rewards, non_score_rewards
#       b.i. for each score (aka r), logprob, ref_logprob, computes KL Divergence as logprob - ref_logprob.
#           Math wise, subtraction of logs is log of their division. It then scales the KL Divergence,
#           and scores it as a non-score-reward and an identical value + score (r) as a reward.
#       b.ii. It then returns the rewards and non_score_rewards            
#   c. For each epoch: --> For each shuffled batch:
#       c.i. Calls train_minibatch(logprob[idx], values[idx], rewards[idx], queries[idx], responses[idx], [query, response])
#           c.i.1. For the logprob, value, reward, query, response, and original model input ([query, response] pair):
#           c.i.2. Calculates loss and runs propagates loss backwards, optimizer takes a step.
#           c.i.3. Loss Calculation(old_logprobs, values, rewards, query(unused), response, model_input)
# 

class ILQLTrainer:
    """
    The ILQLTrainer uses Implicit Language Q Learning to optimise language models.
    """

    default_params = {
        "lr": 1e-4, # From the paper, A.3
        "batch_size": 256,
        "forward_batch_size": 16,
        "visual_dialogue_batch_size": 64,
        "reddit_batch_size": 32,
        "ilql_epochs": 4,
        "gamma": 0.99, # From the paper, A.3
        "alpha": 0.1,
        "tau": 0.7,
        "polyak_decay": 0.005,
        'target_update_frequency': 10, # TODO: Find it from the paper, I pulled this number out of nowhere. 
    }

    def __init__(self, model, tokenizer, **ilql_params):
        """
        Initialize ILQLTrainer.

        Args:
            model (torch.model): Hugging Face transformer GPT2 model with value head
            tokenizer (tokenizer): Hugging Face tokenizer
            ppo_params (dict or None): PPO parameters for training. Can include following keys:
                'lr' (float): Adam learning rate, default: 1.41e-5
                'batch_size' (int): Number of samples per optimisation step, default: 256
                'forward_batch_size' (int): Number of samples forward passed through model at a time, default: 16
                'ilql_epochs' (int): Number of optimisation epochs per batch of samples, default: 4
                'gamma' (float)): Gamma parameter for advantage calculation, default: 1.
                'alpha' (float): Alpha parameter for CQL loss term, default: 0.1
                TODO: Add lambda_Q, lambda_V

        """
        # NOTE: Ilql params are processed here but can have implications in MLP heads' dimensions inside of GPT2, aka, self.model
        self.ilql_params = self.default_params
        self.ilql_params.update(ilql_params)

        self.model: GPT2HeadWithQValueModel = model
        self.tokenizer = tokenizer
        self.data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

        self.optimizer = Adam(model.parameters(), lr=self.ilql_params['lr'])
        self.step_count = 0


    def step(self, queries, responses, scores):
        """
        Run a ILQL optimisation step.

        args:
            queries (List): List of tensors containing the encoded queries, shape [query_length]
            responses (List): List of tensors containing the encoded responses, shape [response_length]
            scores (List): tensor containing the scores, shape [batch_size]

        returns:
            train_stats (dict): a summary of the training statistics
        """
        bs = self.ilql_params['batch_size']
        assert bs == len(queries), f"Batch size ({bs}) does not match number of examples ({len(queries)})"

        timing = dict()
        t0 = time.time()

        t = time.time()
        # Named it _q not q so i can use pdb lol
        
        # NOTE: each output is batchwise! Meaning if BS is Y, values would be Y*(1*response_length), and qs would be Y*(1*response_length*vocab_size) (in the per-token case)
        logprobs, v, q1, q2, target_q, attn_masks = self.batched_forward_pass(queries, responses)
        timing['time/ilql/forward_pass'] = time.time()-t

        t = time.time()
        # TODO: Should this even exist in ILQL? Don't we get rewards from BERT? What does this do?
        # rewards, non_score_reward = self.compute_rewards(scores, logprobs, ref_logprobs)
        # TODO: Since ILQL Rewards are just +ve word sentiments (without any KL divergence term), it is kept
        #  as is.
        rewards = scores
        timing['time/ilql/compute_rewards'] = time.time()-t

        t = time.time()
        all_stats = []
        idxs = list(range(bs))
        for _ in range(self.ilql_params['ilql_epochs']):
            random.shuffle(idxs)
            for i in range(bs):
                idx = idxs[i]
                print(f'Index:{idx} - BS: {bs}')
                # NOTE: Architectural note, why does train_minibatch get applied to each entry vs. batch? what is a minibatch vs batch
                train_stats = self.train_minibatch(logprobs[idx], v[idx],
                                                    [q1[idx], q2[idx]], target_q[idx],
                                                    rewards[idx], queries[idx], responses[idx])
                all_stats.append(train_stats)
        timing['time/ilql/optimize_step'] = time.time()-t

        # Update targets
        if self.step_count + 1 % self.ilql_params['target_update_frequency'] == 0:
            t = time.time()
            # Uses EMA to update target q network
            self.model.soft_update_targets(alpha=self.ilql_params['polyak_decay'])
            train_stats['time/ilql/target_update'] = time.time()-t

        t = time.time()
        train_stats = stack_dicts(all_stats)

        # reshape advantages/ratios such that they are not averaged.
        train_stats['policy/advantages'] = torch.flatten(train_stats['policy/advantages']).unsqueeze(0)
        train_stats['policy/advantages'] = torch.nan_to_num(train_stats['policy/advantages'], WANDB_PADDING)
        train_stats['policy/ratio'] = torch.flatten(train_stats['policy/ratio']).unsqueeze(0)

        # TODO: Update record_step_stats with the right values
        stats = self.record_step_stats(scores=scores, logprobs=logprobs, train_stats=train_stats)
        stats = stats_to_np(stats)
        timing['time/ilql/calc_stats'] = time.time()-t
        timing['time/ilql/total'] = time.time()-t0
        stats.update(timing)
        self.step_count += 1
        print(f'Step: {self.step_count}')
        return stats

    def batched_forward_pass(self, queries, responses):
        """Calculate model outputs in multiple batches."""
        bs = self.ilql_params['batch_size']
        fbs = self.ilql_params['forward_batch_size']
        all_logprobs = []
        all_values = []
        all_q1 = []
        all_q2 = []
        all_target_q = []
        all_attn_masks = []
        # NOTE: Should this exist? I am thinking to have it so I can pass next states to compute target_q
        all_states = []

        for i in range(int(bs/fbs)):
            query_batch = queries[i*fbs:(i+1)*fbs]
            response_batch = responses[i*fbs:(i+1)*fbs]
            input_ids = self.data_collator([torch.cat([q, r]) for q, r in zip(query_batch, response_batch)])["input_ids"]
            # LM OutputW
            # TODO: No grad the forward pass?
            # with torch.no_grad():
            print(f'\tBatched Forward Pass: Running forward pass')
            lmo: CausalLMOutputWithCrossAttentions = self.model(input_ids)
            logits, attn_masks, v, q1, q2, target_q = lmo.logits, lmo.attentions, lmo.value, lmo.qs[0], lmo.qs[1], lmo.target_qs[0]
            # This type of debugging should have me sent straight to hell
            # q = torch.minimum(q1, q2)
            # target_q = torch.minimum(target_q1, target_q2)
            # TODO: What is happening with the indices here?
            logprobs = logprobs_from_logits(logits[:,:-1,:], input_ids[:,1:])
            # TODO: Inspect fbs and bs/fbs etc. to figure out indexing
            print(f'\tBatched Forward Pass; Updating lists')

            for j in range(fbs): 
                start = len(query_batch[j])-1
                end = len(query_batch[j]) + len(response_batch[j])-1
                # TODO: Is this right? Who knows! Where is the indexing from? Is this what's causing the modifications?
                all_q1.append(q1[j, start-1:end-1])
                all_q2.append(q2[j, start-1:end-1])
                all_target_q.append(target_q[j, start-1:end-1])
                all_values.append(v[j, start-1:end-1])
                # TODO: Attn masks are none
                # all_attn_masks.append(attn_masks[j, start-1:end-1]) # Is indexing right? For any of these?
                all_logprobs.append(logprobs[j, start:end])
        print(f'\tBatched Forward Pass updated all_q, all_target_q, all_values, and all_logprobs')
        return all_logprobs, all_values, all_q1, all_q2, all_target_q, all_attn_masks

    # TODO: Arguments need to be changed. 
    # TODO: Check if rewards is 1 value or not.
    def train_minibatch(self, logprobs, values, qs, target_qs, rewards, query, response):
        train_stats = {}
        loss, loss_step_stats = self.ilql_loss(logprobs, values, qs, target_qs, rewards, query, response)
        print(f'Loss version: {loss._version}')
        # TODO: This is just a syntactically correct placeholder I guess
        train_stats.update(loss_step_stats)
        # What's the point of having batches if I will propagate gradients with every input?
        # self.optimizer.zero_grad()
        # loss.backward()
        # self.optimizer.step()
        return train_stats

    def L_QV(self, value, next_value, q, q_hat, reward):
        """
        When batched_forward_pass requires grad:
            next_value.requires_grad:       False
            q.requires_grad :               True
            q_target.requires_grad:         False
            L_Q.requires_grad:              True
            ---
            value.requires_grad:            True
            q_hat.requires_grad:            False
            u.requires_grad:                True
            fancy_one_symbol.requires_grad: False
            expectile.requires_grad:        True
            L_V.requires_grad:              True
            ---
            L_Q.mean().requires_grad:       True
            L_V.mean().requires_grad:       True
        ---
        When batched_forward_pass does not require grad:
            All False
        """
        def _L_doubleQ(q1, q2, _qt):
            q1_loss = (_qt - q1) ** 2
            q2_loss = (_qt - q2) ** 2
            return q1_loss + q2_loss

        # According to https://github.com/Sea-Snell/Implicit-Language-Q-Learning/blob/13e3d58ee27527a0c819c92702d322a829211540/src/models/iql_model.py#L356
        next_value = next_value.detach()
        gamma = self.ilql_params['gamma']
        q_target = reward + (gamma * next_value) # Not to be confused with the result of the target_Q transformer. 
        q1, q2 = q[0], q[1]
        L_Q = _L_doubleQ(q1, q2, q_target)

        # Expectile Regression: - I wrote this to be as readable as possible, not as fast as possible
        q_hat = q_hat.detach()
        u = (value - q_hat) # Q_hat - V, but flipped
        ones, zeros = torch.ones(u.shape), torch.zeros(u.shape)
        fancy_one_symbol = torch.where(u > 0, ones, zeros) # Usually, this is 1 if u < 0, but we flipped it cause we are minimizing and not maximizing, therefore u is a negative number
        tau = self.ilql_params['tau']
        expectile = torch.abs(tau - fancy_one_symbol) * (u ** 2)
        L_V = expectile 

        expectation_L_Q = L_Q.mean()
        expectation_L_V = L_V.mean()
        return expectation_L_Q + expectation_L_V, {}

    def L_CQL(self, q):
        """
        if self.double_q:
            q1, q2 = qs
            b, t, d = q1.shape
            return 
                (
                 (F.cross_entropy(q1.reshape(-1, d) / self.cql_temp, action_tokens.reshape(-1), reduction='none').reshape(b, t)
                 * (1 - terminals[:, :-1])) 
                 +
                 (F.cross_entropy(q2.reshape(-1, d) / self.cql_temp, action_tokens.reshape(-1), reduction='none').reshape(b, t)
                 * (1 - terminals[:, :-1]))
                 )
                 .sum() / max(n.item(), 1.0)
        I am not quite sure what the lines above me mean (https://github.com/Sea-Snell/Implicit-Language-Q-Learning/blob/13e3d58ee27527a0c819c92702d322a829211540/src/models/iql_model.py#L366-L369)
        But I am working on it
        """
        q1, q2 = q[0], q[1]
        numerator1, numerator2 = torch.exp(q1), torch.exp(q2) 
        denominator1, denominator2 = torch.sum(torch.exp(q1)), torch.sum(torch.exp(q2))
        cql1, cql2 = torch.log(torch.divide(numerator1, denominator1)), torch.log(torch.divide(numerator2, denominator2))
        cql = cql1 + cql2
        # We return CQL in expectation 
        return cql.mean(), {}

    # TODO: Figure out args from what Charlie says. ILQL has them as (tokens, attn_mask, logits, w). I found out where to get attn_mask from, but I'm not sure if it's coming
    # from the right transformer (2nd transformer for target_q doesn't exist as of now).

    #  If this is just for PerToken, what are the token arguments in this case?
    # TODO: https://github.com/Sea-Snell/Implicit-Language-Q-Learning/blob/13e3d58ee27527a0c819c92702d322a829211540/src/models/utterance_iql_model.py#L201
    # Utterance ILQL uses AWAC, L_Q, L_V
    # Token ILQL uses AWAC, L_Q, L_V, L_CQL, DM_Loss (What is DM Loss? :-( )
    #  https://github.com/Sea-Snell/Implicit-Language-Q-Learning/blob/13e3d58ee27527a0c819c92702d322a829211540/src/models/iql_model.py#L476
    def L_AWAC(self):
        pass
    
    def ilql_loss(self, old_logprobs, values, double_qs, double_target_qs, reward, query, response):
        # TODO: Remove once (if) you solve the bug
        def _debug(self, l):
            self.optimizer.zero_grad()
            l.backward()
            self.optimizer.step()
        # TODO : Document this as a loss function that gets applied to a single input at a time, as opposed to a batch
        sequence_len = response.shape[0]
        # assert len(values) == len(qs) == len(target_qs) # TODO: Dangerous assertion?
        stats = {}
        # Why have a list of losses if this function is applied to a sentence at a time? How do we have a different loss scalar for each token in
        #  the sequence? Do we just sum them?
        total_loss = 0
        print(f'Sequence length: {sequence_len}')
        for i in range(sequence_len): 
            print(f'i: {i}')
            # SOMEWHERE HERE! V
            # WHY GOD WHY
            next_value = torch.clone(values[i + 1]).detach() if i + 1 < sequence_len else torch.tensor(0.)
            value, double_q, double_target_q = values[i], [double_qs[0][i], double_qs[1][i]], double_target_qs[i]
            qv_loss, qv_stats = self.L_QV(value, next_value, double_q, double_target_q, reward)
            _debug(self, l=qv_loss)
            print(f'QV: {qv_loss}')
            cql_loss, cql_stats = self.L_CQL(double_q)
            _debug(self, l=cql_loss)
            print(f'CQL: {cql_loss:}')
            loss = qv_loss + (self.ilql_params['alpha'] * cql_loss)
            _debug(self, l=loss)
            print(f'Loss: {loss:}')
            total_loss += loss
            _debug(self, l=loss)
            # TODO: Update stats dicts
        return total_loss, flatten_dict(stats)

    # TODO: Repurpose for ILQL
    def record_step_stats(self, kl_coef, **data):
        """Record training step statistics."""
        kl_list = [logprobs-ref_logprobs for logprobs, ref_logprobs in zip(data['logprobs'], data['ref_logprobs'])]
        mean_kl = torch.mean(torch.stack([torch.sum(kl) for kl in kl_list]))
        mean_entropy = torch.mean(torch.stack([torch.sum(-log_probs) for log_probs in data['logprobs']]))
        mean_non_score_reward =torch.mean(torch.stack([torch.sum(non_score_reward) for non_score_reward in data['non_score_reward']]))
        stats = {
            'objective/kl': mean_kl,
            'objective/kl_dist': kl_list,
            'objective/logprobs': data['logprobs'],
            'objective/ref_logprobs': data['ref_logprobs'],
            'objective/kl_coef': kl_coef,
            'objective/entropy': mean_entropy,
            'ppo/mean_non_score_reward': mean_non_score_reward,
        }

        for k, v in data['train_stats'].items():
            stats[f'ppo/{k}'] = torch.mean(v, axis=0)
        stats['ppo/val/var_explained'] = 1 - stats['ppo/val/error'] / stats['ppo/returns/var']
        return stats

