# AUTOGENERATED! DO NOT EDIT! File to edit: nbs/02-ppo.ipynb (unless otherwise specified).

__all__ = ['AdaptiveKLController', 'FixedKLController', 'ILQLTrainer']

# Cell
import numpy as np
import torch.nn.functional as F
from torch.optim import Adam
import torch
import collections
import time
import random

from transformers import DataCollatorForLanguageModeling
from trl.gpt2_ilql import CausalLMOutputWithCrossAttentions, GPT2HeadWithQValueModel

from .core import (logprobs_from_logits,
                      whiten,
                      clip_by_value,
                      entropy_from_logits,
                      flatten_dict,
                      average_torch_dicts,
                      stats_to_np,
                      stack_dicts,
                      add_suffix,
                      WANDB_PADDING)

# Cell
# 1. ILQLTrainer gets initialized with its parameters, a tokenizer, and a model (M).
# 2. The tokenizer is given a query (sentence) to encode into a Tensor Q_T.
# 3. The model M responds to the encoded query Q_T, and the tokenizer decodes the response R_T.
# 4. There is a reward r. Somehow. 
# 5. The ILQLTrainer calls Step on the query and response tensors (Q_T and R_T) and reward r.
# 6. Step(Q_T, R_T, r) -> Stats:
#   a. Runs batched_forward_pass(Q_T, R_T) to get logprobs, ref_logprobs, and values
#       a.i. For each iteration of size BS/FBS, runs the model M on input of [query, response].
#           to return logits, and values. Runs [query, response] through reference model M_r to return
#              reference logits. TODO: Should this return logits, values, and q, for ILQL?
#       a.ii. Logits and reference logits are turned into logprobs and ref_logprobs. Returns
#           lists of logprobs, ref_logprobs, and values
#   b. Runs compute_rewards(scores=r, logprobs, ref_logprobs) to get rewards, non_score_rewards
#       b.i. for each score (aka r), logprob, ref_logprob, computes KL Divergence as logprob - ref_logprob.
#           Math wise, subtraction of logs is log of their division. It then scales the KL Divergence,
#           and scores it as a non-score-reward and an identical value + score (r) as a reward.
#       b.ii. It then returns the rewards and non_score_rewards            
#   c. For each epoch: --> For each shuffled batch:
#       c.i. Calls train_minibatch(logprob[idx], values[idx], rewards[idx], queries[idx], responses[idx], [query, response])
#           c.i.1. For the logprob, value, reward, query, response, and original model input ([query, response] pair):
#           c.i.2. Calculates loss and runs propagates loss backwards, optimizer takes a step.
#           c.i.3. Loss Calculation(old_logprobs, values, rewards, query(unused), response, model_input)
# 

class ILQLTrainer:
    """
    The ILQLTrainer uses Implicit Language Q Learning to optimise language models.
    """

    default_params = {
        "lr": 1e-4, # From the paper, A.3
        "batch_size": 256,
        "forward_batch_size": 16,
        "visual_dialogue_batch_size": 64,
        "reddit_batch_size": 32,
        "ilql_epochs": 4,
        "gamma": 0.99, # From the paper, A.3
        "alpha": 0.1,
        "tau": 0.7,
        "polyak_decay": 0.005,
        'target_update_frequency': 10, # TODO: Find it from the paper, I pulled this number out of nowhere. 
    }

    def __init__(self, model, tokenizer, **ilql_params):
        """
        Initialize ILQLTrainer.

        Args:
            model (torch.model): Hugging Face transformer GPT2 model with value head
            tokenizer (tokenizer): Hugging Face tokenizer
            ppo_params (dict or None): PPO parameters for training. Can include following keys:
                'lr' (float): Adam learning rate, default: 1.41e-5
                'batch_size' (int): Number of samples per optimisation step, default: 256
                'forward_batch_size' (int): Number of samples forward passed through model at a time, default: 16
                'ilql_epochs' (int): Number of optimisation epochs per batch of samples, default: 4
                'gamma' (float)): Gamma parameter for advantage calculation, default: 1.
                'alpha' (float): Alpha parameter for CQL loss term, default: 0.1
                TODO: Add lambda_Q, lambda_V

        """
        # NOTE: Ilql params are processed here but can have implications in MLP heads' dimensions inside of GPT2, aka, self.model
        self.ilql_params = self.default_params
        self.ilql_params.update(ilql_params)

        self.model: GPT2HeadWithQValueModel = model
        self.tokenizer = tokenizer
        self.data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)

        self.optimizer = Adam(model.parameters(), lr=self.ilql_params['lr'])
        self.step_count = 0

    def step(self, queries, responses, scores):
        """
        Run a ILQL optimisation step.

        args:
            queries (List): List of tensors containing the encoded queries, shape [query_length]
            responses (List): List of tensors containing the encoded responses, shape [response_length]
            scores (List): tensor containing the scores, shape [batch_size]

        returns:
            train_stats (dict): a summary of the training statistics
        """
        bs = self.ilql_params['batch_size']
        assert bs == len(queries), f"Batch size ({bs}) does not match number of examples ({len(queries)})"

        timing = dict()
        t0 = time.time()

        response_lengths = [len(r) for r in responses]

        t = time.time()
        # Named it _q not q so i can use pdb lol
        
        # NOTE: each output is batchwise! Meaning if BS is Y, values would be Y*(1*response_length), and qs would be Y*(1*response_length*vocab_size) (in the per-token case)
        logprobs, v, _q, target_q, attn_masks = self.batched_forward_pass(queries, responses)
        timing['time/ilql/forward_pass'] = time.time()-t

        t = time.time()
        # TODO: Should this even exist in ILQL? Don't we get rewards from BERT? What does this do?
        # rewards, non_score_reward = self.compute_rewards(scores, logprobs, ref_logprobs)
        # TODO: Since ILQL Rewards are just +ve word sentiments (without any KL divergence term), it is kept
        #  as is.
        rewards = scores
        timing['time/ilql/compute_rewards'] = time.time()-t

        t = time.time()
        all_stats = []
        idxs = list(range(bs))
        for _ in range(self.ilql_params['ilql_epochs']):
            random.shuffle(idxs)
            for i in range(bs):
                idx = idxs[i]
                # TODO: Change function arguments. 
                # Refer to https://github.dev/rail-berkeley/rlkit/blob/master/rlkit/torch/sac/iql_trainer.py
                # I think this needs: 
                #  q_pred (detached): min(model.tq1(s, a), model.tq2(s, a)),
                #  q1_pred, q2_pred <-- self.q1(s, a)
                #  v: model.v(s),
                print(f'Index:{idx} - BS: {bs}')
                # NOTE: Architectural note, why does train_minibatch get applied to each entry vs. batch? what is a minibatch vs batch
                # Why do we unsqueeze(0) if train_minibatch applies it to each input? why [, length] -> [1, length]? 
                train_stats = self.train_minibatch(logprobs[idx], v[idx],
                                                    _q[idx], target_q[idx], rewards[idx],
                                                    queries[idx], responses[idx], 
                                                    torch.cat([queries[idx],responses[idx]]))
                all_stats.append(train_stats)
        timing['time/ilql/optimize_step'] = time.time()-t

        # Update targets
        if self.step_count + 1 % self.ilql_params['target_update_frequency'] == 0:
            t = time.time()
            # Uses EMA to update target q network
            self.model.soft_update_targets(alpha=self.ilql_params['polyak_decay'])
            train_stats['time/ilql/target_update'] = time.time()-t

        t = time.time()
        train_stats = stack_dicts(all_stats)

        # reshape advantages/ratios such that they are not averaged.
        train_stats['policy/advantages'] = torch.flatten(train_stats['policy/advantages']).unsqueeze(0)
        train_stats['policy/advantages'] = torch.nan_to_num(train_stats['policy/advantages'], WANDB_PADDING)
        train_stats['policy/ratio'] = torch.flatten(train_stats['policy/ratio']).unsqueeze(0)

        # TODO: Update record_step_stats with the right values
        stats = self.record_step_stats(scores=scores, logprobs=logprobs, train_stats=train_stats)
        stats = stats_to_np(stats)
        timing['time/ilql/calc_stats'] = time.time()-t
        timing['time/ilql/total'] = time.time()-t0
        stats.update(timing)
        self.step_count += 1
        return stats

    def batched_forward_pass(self, queries, responses):
        """Calculate model outputs in multiple batches."""
        bs = self.ilql_params['batch_size']
        fbs = self.ilql_params['forward_batch_size']
        all_logprobs = []
        all_values = []
        all_q = []
        all_target_q = []
        all_attn_masks = []
        # NOTE: Should this exist? I am thinking to have it so I can pass next states to compute target_q
        all_states = []

        for i in range(int(bs/fbs)):
            query_batch = queries[i*fbs:(i+1)*fbs]
            response_batch = responses[i*fbs:(i+1)*fbs]
            input_ids = self.data_collator([torch.cat([q, r]) for q, r in zip(query_batch, response_batch)])["input_ids"]
            # LM Output
            # TODO: No grad the forward pass?
            lmo: CausalLMOutputWithCrossAttentions = self.model(input_ids)
            logits, attn_masks, v, q1, q2, target_q1, target_q2 = lmo.logits, lmo.attentions, lmo.value, lmo.qs[0], lmo.qs[1], lmo.target_qs[0], lmo.target_qs[1]
            q = torch.minimum(q1, q2)
            target_q = torch.minimum(target_q1, target_q2)
            # TODO: What is happening with the indices here?
            logprobs = logprobs_from_logits(logits[:,:-1,:], input_ids[:,1:])
            # TODO: Inspect fbs and bs/fbs etc. to figure out indexing
            for j in range(fbs): 
                start = len(query_batch[j])-1
                end = len(query_batch[j]) + len(response_batch[j])-1
                # TODO: Is this right? Who knows! Where is the indexing from?
                all_q.append(q[j, start-1:end-1])
                all_target_q.append(target_q[j, start-1:end-1])
                all_values.append(v[j, start-1:end-1])
                # TODO: Attn masks are none
                # all_attn_masks.append(attn_masks[j, start-1:end-1]) # Is indexing right? For any of these?
                all_logprobs.append(logprobs[j, start:end])

        return all_logprobs, all_values, all_q, all_target_q, all_attn_masks

    # TODO: Arguments need to be changed. 
    # TODO: Check if rewards is 1 value or not.
    def train_minibatch(self, logprobs, values, qs, target_qs, rewards, query, response, model_input):
        train_stats = {}
        loss, loss_step_stats = self.ilql_loss(logprobs, values, qs, target_qs, rewards, query, response, model_input)
        # Experimental haha
        loss_scalar = loss.sum()
        # TODO: This is just a syntactically correct placeholder I guess
        train_stats.update(loss_step_stats)
        # What's the point of having batches if I will propagate gradients with every input?
        self.optimizer.zero_grad()
        loss_scalar.backward()
        self.optimizer.step()
        return train_stats

    def L_QV(self, value, next_value, q, q_hat, reward):
        # R(s, a) + V(s_i+1) - Q(s, a) <-- squared
        # +
        # Expectile loss(Q_hat(s, a) - V(s) )
        # TODO - Should this be passed as an arg?
        gamma = self.ilql_params['gamma']
        q_target = reward + (gamma * next_value) # Not to be confused with the result of the target_Q transformer. 
        L_Q = (q_target - q) ** 2
        # Expectile Regression: - I wrote this to be as readable as possible.
        u = (value - q_hat) # Q_hat - V, but flipped
        ones, zeros = torch.ones(u.shape), torch.zeros(u.shape)
        fancy_one_symbol = torch.where(u > 0, ones, zeros) # Usually, this is 1 if u < 0, but we flipped it cause we are minimizing and not maximizing, therefore u is a negative number
        tau = self.ilql_params['tau']
        expectile = torch.abs(tau - fancy_one_symbol) * (u ** 2)
        L_V = expectile # In the IQL implementation, it is averaged with .mean(), but I think that's cause the loss
        # is applied on the whole batch at the same time (like, implementation wise), whereas the loss fn here takes 1 entry at a time
        return L_Q + L_V, {}

    def L_CQL(self, q):
        """
        if self.double_q:
            q1, q2 = qs
            b, t, d = q1.shape
            return 
                (
                 (F.cross_entropy(q1.reshape(-1, d) / self.cql_temp, action_tokens.reshape(-1), reduction='none').reshape(b, t)
                 * (1 - terminals[:, :-1])) 
                 +
                 (F.cross_entropy(q2.reshape(-1, d) / self.cql_temp, action_tokens.reshape(-1), reduction='none').reshape(b, t)
                 * (1 - terminals[:, :-1]))
                 )
                 .sum() / max(n.item(), 1.0)
        I am not quite sure what the lines above me mean (https://github.com/Sea-Snell/Implicit-Language-Q-Learning/blob/13e3d58ee27527a0c819c92702d322a829211540/src/models/iql_model.py#L366-L369)
        But I am working on it
        """
        # TODO: How do I get action tokens? Is it from GPT2? Maybe its already part of q, from the Q Head hidden size? but that wouldn't make sense
        # EDIT: Turns out I don't need them for per utterance, only for pertoken. I am not sure which version of ILQL we are doing but I will implement both.
        #  For PerToken, previous comment is right - it is part of Q Head's hidden size that is configured to be vocab_size, particularly for PerToken, which is 
        #  what uses CCE Loss over the actions (or vocab)
        # TODO: Now all that's left is to set a breakpoint here and call cross entropy on the right Qs:or just
        # TODO: What is q and what is qs in PerToken example?
        numerator = torch.exp(q) # Q of the current action - how do I get that?
        denominator = torch.sum(
            torch.exp(q) # Is q<--Q(s,a) and qs a listlike of q(s,a'), forall a' in A? OR: Is q a 1*vocab_size tensor?
        )
        cql = torch.log(
            torch.divide(numerator, denominator)
        )
        # We return CQL in expectation 
        return cql.mean(), {}

    # TODO: Figure out args from what Charlie says. ILQL has them as (tokens, attn_mask, logits, w). I found out where to get attn_mask from, but I'm not sure if it's coming
    # from the right transformer (2nd transformer for target_q doesn't exist as of now).

    #  If this is just for PerToken, what are the token arguments in this case?
    # TODO: https://github.com/Sea-Snell/Implicit-Language-Q-Learning/blob/13e3d58ee27527a0c819c92702d322a829211540/src/models/utterance_iql_model.py#L201
    # Utterance ILQL uses AWAC, L_Q, L_V
    # Token ILQL uses AWAC, L_Q, L_V, L_CQL, DM_Loss (What is DM Loss? :-( )
    #  https://github.com/Sea-Snell/Implicit-Language-Q-Learning/blob/13e3d58ee27527a0c819c92702d322a829211540/src/models/iql_model.py#L476
    def L_AWAC(self):
        pass
    
    def ilql_loss(self, old_logprobs, values, qs, target_qs, reward, query, response, model_input):
        # TODO : Document this as a loss function that gets applied to a single input at a time, as opposed to a batch
        sequence_len = response.shape[0]
        # assert len(values) == len(qs) == len(target_qs) # TODO: Dangerous assertion?
        stats = {}
        # Why have a list of losses if this function is applied to a sentence at a time? How do we have a different loss scalar for each token in
        #  the sequence? Do we just sum them?
        total_loss = 0
        for i in range(sequence_len): 
            next_value = values[i + 1] if i + 1 < sequence_len else 0.
            value, q, target_q = values[i], qs[i], target_qs[i]
            qv_loss, qv_stats = self.L_QV(value, next_value, q, target_q, reward)
            cql_loss, cql_stats = self.L_CQL(q)
            loss = qv_loss + (self.ilql_params['alpha'] * cql_loss)
            total_loss += loss
            # TODO: Update stats dicts
        # TODO: In the IQL implementation, batch-wise loss is averaged via .mean(). What should we do here? 
        #  See: https://github.com/rail-berkeley/rlkit/blob/master/rlkit/torch/sac/iql_trainer.py#L166
        return total_loss, flatten_dict(stats)

    # TODO: Repurpose for ILQL
    def record_step_stats(self, kl_coef, **data):
        """Record training step statistics."""
        kl_list = [logprobs-ref_logprobs for logprobs, ref_logprobs in zip(data['logprobs'], data['ref_logprobs'])]
        mean_kl = torch.mean(torch.stack([torch.sum(kl) for kl in kl_list]))
        mean_entropy = torch.mean(torch.stack([torch.sum(-log_probs) for log_probs in data['logprobs']]))
        mean_non_score_reward =torch.mean(torch.stack([torch.sum(non_score_reward) for non_score_reward in data['non_score_reward']]))
        stats = {
            'objective/kl': mean_kl,
            'objective/kl_dist': kl_list,
            'objective/logprobs': data['logprobs'],
            'objective/ref_logprobs': data['ref_logprobs'],
            'objective/kl_coef': kl_coef,
            'objective/entropy': mean_entropy,
            'ppo/mean_non_score_reward': mean_non_score_reward,
        }

        for k, v in data['train_stats'].items():
            stats[f'ppo/{k}'] = torch.mean(v, axis=0)
        stats['ppo/val/var_explained'] = 1 - stats['ppo/val/error'] / stats['ppo/returns/var']
        return stats

